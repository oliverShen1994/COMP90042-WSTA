{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"wsta-train-model.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"Q6GlAMNQBOS-","colab_type":"code","colab":{}},"source":["import datetime\n","import json\n","import os\n","import pprint\n","import random\n","import string\n","import sys\n","import pprint\n","import tensorflow as tf\n","\n","if 'COLAB_TPU_ADDR' not in os.environ:\n","  print('ERROR: Not connected to a TPU runtime')\n","else:\n","  TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n","  print ('TPU address is', TPU_ADDRESS)\n","\n","from google.colab import auth\n","auth.authenticate_user()\n","with tf.Session(TPU_ADDRESS) as session:\n","  print('TPU devices:')\n","  pprint.pprint(session.list_devices())\n","\n","  # Upload credentials to TPU.\n","  with open('/content/adc.json', 'r') as f:\n","    auth_info = json.load(f)\n","  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q7HcKULWCTJd","colab_type":"code","colab":{}},"source":["!pip install bert-tensorflow"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6K-zWUs_nHsO","colab_type":"text"},"source":["## Download fine-tune BERT file and get authority for Google drive:"]},{"cell_type":"code","metadata":{"id":"r7PC7mn8nZAL","colab_type":"code","colab":{}},"source":["!test -d bert_model_repo || git clone https://github.com/google-research/bert bert_model_repo\n","if not 'bert_model_repo' in sys.path:\n","  sys.path += ['bert_model_repo']\n","\n","# import python modules defined by BERT\n","import modeling\n","import optimization\n","import run_classifier\n","import run_classifier_with_tfhub\n","import tokenization\n","from run_classifier import InputExample\n","\n","# import tfhub \n","import tensorflow_hub as hub\n","\n","#Get authority for google drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kU8NLL_9ClmR","colab_type":"text"},"source":["## Load our own processors:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"H9kEIdNPbmso","colab":{}},"source":["# Trained for sentence selection\n","class WstasentenceProcessor(run_classifier.DataProcessor):\n","  \"\"\"Processor for the WSTA data set to select sentences.\"\"\"\n","  def get_train_examples(self, data_dir):\n","    \"\"\"See base class.\"\"\"\n","    return self._create_examples(\n","        self._read_tsv(os.path.join(data_dir, \"trainset_for_model.tsv\")), \"train\")\n","\n","  def get_dev_examples(self, data_dir):\n","    \"\"\"See base class.\"\"\"\n","    return self._create_examples(\n","        self._read_tsv(os.path.join(data_dir, \"devset_for_model.tsv\")), \"dev\")\n","\n","  def get_test_examples(self, data_dir, filename):\n","    \"\"\"See base class.\"\"\"\n","    return self._create_examples(\n","        self._read_tsv(os.path.join(data_dir, filename)), \"test\")\n","\n","  def get_labels(self):\n","    \"\"\"See base class.\"\"\"\n","    return ['1', '0']\n","\n","  def _create_examples(self, lines, set_type):\n","    examples = []\n","    for (i, line) in enumerate(lines):\n","      if i == 0:\n","        continue\n","      guid = \"%s-%s\" % (set_type, i)\n","      text_a = tokenization.convert_to_unicode(line[0])\n","      text_b = tokenization.convert_to_unicode(line[1])\n","      if set_type == \"test\":\n","        label = \"0\"\n","      else:\n","        label = tokenization.convert_to_unicode(line[2])\n","      examples.append(\n","          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n","    return examples\n","      \n","\n","#Trained for classification\n","class WstalabelProcessor(run_classifier.DataProcessor):\n","    \"\"\"Processor for the WSTA data set to classify label.\"\"\"\n","\n","    def get_train_examples(self, data_dir):\n","      return self._create_examples(\n","        self._read_tsv(os.path.join(data_dir, \"train_classification.tsv\")), \"train\")\n","\n","    def get_dev_examples(self, data_dir):\n","      return self._create_examples(\n","        self._read_tsv(os.path.join(data_dir, \"dev_classification.tsv\")), \"dev\")\n","\n","    def get_test_examples(self, data_dir, filename):\n","      return self._create_examples(\n","        self._read_tsv(os.path.join(data_dir, filename)), \"test\")\n","\n","    def get_labels(self):\n","      return [\"SUPPORTS\",\"REFUTES\",\"NOT ENOUGH INFO\"]\n","    \n","    def _create_examples(self, lines, set_type):\n","      examples = []\n","      for (i, line) in enumerate(lines):\n","        if i == 0:\n","          continue\n","        guid = \"%s-%s\" % (set_type, i)\n","        text_a = tokenization.convert_to_unicode(line[0])\n","        text_b = tokenization.convert_to_unicode(line[1])\n","        if set_type == \"test\":\n","          label = \"NOT ENOUGH INFO\"\n","        else:\n","          label = tokenization.convert_to_unicode(line[2])\n","        examples.append(\n","            InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n","      return examples"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UC0VFATHB8tj","colab_type":"text"},"source":["## Load model and config:"]},{"cell_type":"code","metadata":{"id":"_X0ZPf_WCCVo","colab_type":"code","colab":{}},"source":["TRAIN_BATCH_SIZE = 24\n","EVAL_BATCH_SIZE = 8\n","PREDICT_BATCH_SIZE = 8\n","LEARNING_RATE = 2e-5\n","NUM_TRAIN_EPOCHS = 2.0\n","MAX_SEQ_LENGTH = 128\n","# Used to help training\n","WARMUP_PROPORTION = 0.1\n","# Model configs\n","SAVE_CHECKPOINTS_STEPS = 1000\n","SAVE_SUMMARY_STEPS = 500\n","\n","processors = {\n","  \"wstasentence\": WstasentenceProcessor,\n","  \"wstalabel\": WstalabelProcessor,\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c151L2VQI4rw","colab_type":"text"},"source":["## 1. Sentence selection:"]},{"cell_type":"markdown","metadata":{"id":"UskpxOSgXVTs","colab_type":"text"},"source":["#### Configuration:"]},{"cell_type":"code","metadata":{"id":"wca8r-p-I2Pj","colab_type":"code","colab":{}},"source":["TASK = 'wstasentence'\n","assert TASK in ('wstasentence', 'wstalabel')\n","\n","BUCKET = 'colab-storage'\n","TASK_DATA_DIR = '/content/gdrive/My Drive'\n","print('Task data directory: {}'.format(TASK_DATA_DIR))\n","#!ls $TASK_DATA_DIR\n","\n","OUTPUT_DIR = 'gs://{}/bert-models/{}'.format(BUCKET, TASK)\n","tf.gfile.MakeDirs(OUTPUT_DIR) #model output dir\n","# Force TF Hub writes to the GS bucket we provide.\n","os.environ['TFHUB_CACHE_DIR'] = OUTPUT_DIR\n","\n","BERT_MODEL = 'uncased_L-12_H-768_A-12'\n","BERT_MODEL_HUB = 'https://tfhub.dev/google/bert_' + BERT_MODEL + '/1'\n","\n","tokenizer = run_classifier_with_tfhub.create_tokenizer_from_hub_module(BERT_MODEL_HUB)\n","\n","processor = processors[TASK.lower()]()\n","label_list = processor.get_labels()\n","\n","# Compute number of train and warmup steps from batch size\n","train_examples = processor.get_train_examples(TASK_DATA_DIR)\n","\n","num_train_steps = int(len(train_examples) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n","num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n","\n","# TPU config\n","tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n","NUM_TPU_CORES = 8\n","ITERATIONS_PER_LOOP = 1000"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RmfpsLUO9z7v","colab_type":"code","colab":{}},"source":["#Estimator config\n","def get_run_config(output_dir):\n","  return tf.contrib.tpu.RunConfig(\n","    cluster=tpu_cluster_resolver,\n","    model_dir=output_dir,\n","    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n","    tpu_config=tf.contrib.tpu.TPUConfig(\n","        iterations_per_loop=ITERATIONS_PER_LOOP,\n","        num_shards=NUM_TPU_CORES,\n","        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n","\n","model_fn = run_classifier_with_tfhub.model_fn_builder(\n","  use_tpu=True,\n","  num_labels=len(label_list),\n","  learning_rate=LEARNING_RATE,\n","  num_train_steps=num_train_steps,\n","  num_warmup_steps=num_warmup_steps,\n","  bert_hub_module_handle=BERT_MODEL_HUB\n",")\n","\n","estimator_from_tfhub = tf.contrib.tpu.TPUEstimator(\n","  use_tpu=True,\n","  model_fn=model_fn,\n","  config=get_run_config(OUTPUT_DIR),\n","  train_batch_size=TRAIN_BATCH_SIZE,\n","  eval_batch_size=EVAL_BATCH_SIZE,\n","  predict_batch_size=PREDICT_BATCH_SIZE,\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"678Y4YXJYrVW","colab_type":"text"},"source":["#### Train, evaluate and predict:"]},{"cell_type":"code","metadata":{"id":"o6-tcSRcYxbE","colab_type":"code","colab":{}},"source":["# Train the model\n","def model_train(estimator):\n","  train_features = run_classifier.convert_examples_to_features(\n","      train_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","  print('Started training at {}'.format(datetime.datetime.now()))\n","  print('Num examples = {}'.format(len(train_examples)))\n","  tf.logging.info(\"Num steps = %d\", num_train_steps)\n","  train_input_fn = run_classifier.input_fn_builder(\n","      features=train_features,\n","      seq_length=MAX_SEQ_LENGTH,\n","      is_training=True,\n","      drop_remainder=True)\n","  estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n","  print('Finished training at {}'.format(datetime.datetime.now()))\n","  \n","  \n","# Eval the model.\n","def model_eval(estimator):\n","  eval_examples = processor.get_dev_examples(TASK_DATA_DIR)\n","  eval_features = run_classifier.convert_examples_to_features(\n","      eval_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","  print('Started evaluation at {}'.format(datetime.datetime.now()))\n","  print('Num examples = {}'.format(len(eval_examples)))\n","  eval_steps = int(len(eval_examples) / EVAL_BATCH_SIZE)\n","  eval_input_fn = run_classifier.input_fn_builder(\n","      features=eval_features,\n","      seq_length=MAX_SEQ_LENGTH,\n","      is_training=False,\n","      drop_remainder=True)\n","  result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n","  print('Finished evaluation at {}'.format(datetime.datetime.now()))\n","  output_eval_file = os.path.join(OUTPUT_DIR, \"eval_results.txt\")\n","  with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n","    print(\"***** Eval results *****\")\n","    for key in sorted(result.keys()):\n","      print('  {} = {}'.format(key, str(result[key])))\n","      writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n","      \n","#Predict\n","def model_predict(estimator):\n","  #Due to delayed submission of codalab, this part of code alternatively uses dev set as test set for alternative evaluation:\n","  #prediction_examples = processor.get_test_examples(TASK_DATA_DIR,\"devset_usedfortest_for_model.tsv\")\n","  \n","  #use original test file\n","  prediction_examples = processor.get_test_examples(TASK_DATA_DIR,\"testset_for_model.tsv\")\n","  input_features = run_classifier.convert_examples_to_features(prediction_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=True)\n","  predictions = estimator.predict(predict_input_fn)\n","  #return predictions\n","  for example, prediction in zip(prediction_examples, predictions):\n","    #Due to delayed submission of codalab, this part of code alternatively uses dev set as test set for alternative evaluation:\n","    #output_test_file = os.path.join(OUTPUT_DIR, \"selection_devastest_results.txt\")\n","    \n","    #use original test file\n","    output_test_file = os.path.join(OUTPUT_DIR, \"selection_test_results.txt\")\n","    with tf.gfile.GFile(output_test_file, \"w\") as writer:\n","      for prediction in predictions:\n","        writer.write(\"{}\\n\".format(prediction['probabilities']))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e2_LNs1T4gOh","colab_type":"code","colab":{}},"source":["model_train(estimator_from_tfhub)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eUsCuG_84yUp","colab_type":"code","colab":{}},"source":["model_eval(estimator_from_tfhub)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PaRoJ-pd2Zpy","colab_type":"code","colab":{}},"source":["model_predict(estimator_from_tfhub)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fWoaQY1fcSKG","colab_type":"text"},"source":["## 2. Classification:"]},{"cell_type":"markdown","metadata":{"id":"dRR9FbD1ce9d","colab_type":"text"},"source":["#### Configuration:"]},{"cell_type":"code","metadata":{"id":"6k7CPYwEceel","colab_type":"code","colab":{}},"source":["TASK = 'wstalabel'\n","assert TASK in ('wstasentence', 'wstalabel')\n","\n","BUCKET = 'colab-storage'\n","TASK_DATA_DIR = '/content/gdrive/My Drive'\n","print('Task data directory: {}'.format(TASK_DATA_DIR))\n","#!ls $TASK_DATA_DIR\n","\n","OUTPUT_DIR = 'gs://{}/bert-models/{}'.format(BUCKET, TASK)\n","tf.gfile.MakeDirs(OUTPUT_DIR) #model output dir\n","# Force TF Hub writes to the GS bucket we provide.\n","os.environ['TFHUB_CACHE_DIR'] = OUTPUT_DIR\n","\n","BERT_MODEL = 'uncased_L-12_H-768_A-12'\n","BERT_MODEL_HUB = 'https://tfhub.dev/google/bert_' + BERT_MODEL + '/1'\n","\n","tokenizer = run_classifier_with_tfhub.create_tokenizer_from_hub_module(BERT_MODEL_HUB)\n","\n","processor = processors[TASK.lower()]()\n","label_list = processor.get_labels()\n","\n","# Compute number of train and warmup steps from batch size\n","train_examples = processor.get_train_examples(TASK_DATA_DIR)\n","\n","num_train_steps = int(len(train_examples) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n","num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n","\n","# TPU config\n","tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n","NUM_TPU_CORES = 8\n","ITERATIONS_PER_LOOP = 1000"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wy2Mb2js1-at","colab_type":"code","colab":{}},"source":["#Estimator config\n","def get_run_config(output_dir):\n","  return tf.contrib.tpu.RunConfig(\n","    cluster=tpu_cluster_resolver,\n","    model_dir=output_dir,\n","    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n","    tpu_config=tf.contrib.tpu.TPUConfig(\n","        iterations_per_loop=ITERATIONS_PER_LOOP,\n","        num_shards=NUM_TPU_CORES,\n","        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n","\n","model_fn = run_classifier_with_tfhub.model_fn_builder(\n","  use_tpu=True,\n","  num_labels=len(label_list),\n","  learning_rate=LEARNING_RATE,\n","  num_train_steps=num_train_steps,\n","  num_warmup_steps=num_warmup_steps,\n","  bert_hub_module_handle=BERT_MODEL_HUB\n",")\n","\n","estimator_from_tfhub = tf.contrib.tpu.TPUEstimator(\n","  use_tpu=True,\n","  model_fn=model_fn,\n","  config=get_run_config(OUTPUT_DIR),\n","  train_batch_size=TRAIN_BATCH_SIZE,\n","  eval_batch_size=EVAL_BATCH_SIZE,\n","  predict_batch_size=PREDICT_BATCH_SIZE,\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ftSbaa8e2RVa","colab_type":"text"},"source":["#### Train, evaluate and predict:"]},{"cell_type":"code","metadata":{"id":"kmxXcgtP2Qfd","colab_type":"code","colab":{}},"source":["# Train the model\n","def model_train(estimator):\n","  train_features = run_classifier.convert_examples_to_features(\n","      train_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","  print('Started training at {}'.format(datetime.datetime.now()))\n","  print('Num examples = {}'.format(len(train_examples)))\n","  tf.logging.info(\"Num steps = %d\", num_train_steps)\n","  train_input_fn = run_classifier.input_fn_builder(\n","      features=train_features,\n","      seq_length=MAX_SEQ_LENGTH,\n","      is_training=True,\n","      drop_remainder=True)\n","  estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n","  print('Finished training at {}'.format(datetime.datetime.now()))\n","  \n","  \n","# Eval the model.\n","def model_eval(estimator):\n","  eval_examples = processor.get_dev_examples(TASK_DATA_DIR)\n","  eval_features = run_classifier.convert_examples_to_features(\n","      eval_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","  print('Started evaluation at {}'.format(datetime.datetime.now()))\n","  print('Num examples = {}'.format(len(eval_examples)))\n","  # Eval will be slightly WRONG on the TPU because it will truncate the last batch.\n","  eval_steps = int(len(eval_examples) / EVAL_BATCH_SIZE)\n","  eval_input_fn = run_classifier.input_fn_builder(\n","      features=eval_features,\n","      seq_length=MAX_SEQ_LENGTH,\n","      is_training=False,\n","      drop_remainder=True)\n","  result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n","  print('Finished evaluation at {}'.format(datetime.datetime.now()))\n","  output_eval_file = os.path.join(OUTPUT_DIR, \"eval_results.txt\")\n","  with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n","    print(\"***** Eval results *****\")\n","    for key in sorted(result.keys()):\n","      print('  {} = {}'.format(key, str(result[key])))\n","      writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n","      \n","#Predict\n","def model_predict(estimator):\n","  #Due to delayed submission of codalab, this part of code alternatively uses dev set as test set for alternative evaluation:\n","  #prediction_examples = processor.get_test_examples(TASK_DATA_DIR,\"devastest_classification.tsv\")\n","  \n","  #use original test file\n","  prediction_examples = processor.get_test_examples(TASK_DATA_DIR,\"test_classification.tsv\")\n","  input_features = run_classifier.convert_examples_to_features(prediction_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=True)\n","  predictions = estimator.predict(predict_input_fn)\n","  #return predictions\n","  for example, prediction in zip(prediction_examples, predictions):\n","    #Due to delayed submission of codalab, this part of code alternatively uses dev set as test set for alternative evaluation:\n","    #output_test_file = os.path.join(OUTPUT_DIR, \"classification_devastest_results.txt\")\n","    \n","    #use original test file\n","    output_test_file = os.path.join(OUTPUT_DIR, \"classification_test_results.txt\")\n","    with tf.gfile.GFile(output_test_file, \"w\") as writer:\n","      for prediction in predictions:\n","        writer.write(\"{}\\n\".format(prediction['probabilities']))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yITVwICk3SBW","colab_type":"code","colab":{}},"source":["model_train(estimator_from_tfhub)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gU_eZQxL3s9v","colab_type":"code","colab":{}},"source":["model_eval(estimator_from_tfhub)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"D_7VhJskTTFy","colab_type":"code","colab":{}},"source":["model_predict(estimator_from_tfhub)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8OKTqcgRTeTF","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}