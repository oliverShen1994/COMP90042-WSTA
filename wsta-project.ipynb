{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fact Verification System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, store all sets and preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import json\n",
    "import unicodedata\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "import csv\n",
    "\n",
    "with open(\"train.json\",\"r\",encoding='utf8') as file:\n",
    "    train_set = json.load(file)\n",
    "with open(\"devset.json\",\"r\",encoding='utf8') as file:\n",
    "    dev_set = json.load(file)\n",
    "with open(\"test-unlabelled.json\",\"r\",encoding='utf8') as file:\n",
    "    test_set = json.load(file)\n",
    "    \n",
    "#Use NFD normalization to normalize Latin character to be the same as wiki document name\n",
    "for idx,data in train_set.items():\n",
    "    for evidence in data['evidence']:\n",
    "        evidence[0] = unicodedata.normalize('NFC',evidence[0]) \n",
    "for idx,data in dev_set.items():\n",
    "    for evidence in data['evidence']:\n",
    "        evidence[0] = unicodedata.normalize('NFC',evidence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_set))\n",
    "print(len(dev_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract wiki file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"wiki-pages-text\"):\n",
    "    wiki_zip = zipfile.ZipFile(\"wiki-pages-text.zip\")\n",
    "    wiki_zip.extractall()\n",
    "    wiki_zip.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_content = defaultdict(dict)\n",
    "for wiki_top, dirnames, filenames in os.walk(\"wiki-pages-text\"):\n",
    "    for filename in filenames:\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        file = open(os.path.join(wiki_top,filename))\n",
    "        sentences = file.readlines()\n",
    "        file.close()\n",
    "        for sentence in sentences:\n",
    "            sentence_list = sentence.strip().split(\" \",2)\n",
    "            try:\n",
    "                senten_num = int(sentence_list[1]) \n",
    "            except ValueError:\n",
    "                #senten_num doesn't exist, \n",
    "                #data format is wrong and should be given up;\n",
    "                #sum of valid sentence: 25247896\n",
    "                #sum of total sentences in wiki_doc: 25248397\n",
    "                continue\n",
    "            wiki_name = sentence_list[0]\n",
    "            wiki_name = ' '.join(wiki_name.split('_'))\n",
    "            senten_cont = sentence_list[2]\n",
    "            wiki_content[wiki_name][str(senten_num)] = senten_cont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second, Index construction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pylucene index construction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import lucene\n",
    "from collections import defaultdict\n",
    "from java.io import File\n",
    "from org.apache.lucene.document import Document, Field, FieldType\n",
    "from org.apache.lucene.analysis.standard import StandardAnalyzer\n",
    "from org.apache.lucene.index import IndexWriter,IndexWriterConfig,IndexOptions\n",
    "from org.apache.lucene.store import SimpleFSDirectory\n",
    "from datetime import datetime\n",
    "\n",
    "#construct wiki_index \n",
    "def make_wiki_index(index_dir, wiki_doc_dir, analyzer, wiki_content):\n",
    "    if not os.path.exists(index_dir):\n",
    "        os.mkdir(index_dir)\n",
    "    store_dir = SimpleFSDirectory(File(index_dir).toPath())\n",
    "    writerConfig = IndexWriterConfig(StandardAnalyzer())\n",
    "    #every time start this function, the index will be built from the beginning\n",
    "    writerConfig.setOpenMode(IndexWriterConfig.OpenMode.CREATE)\n",
    "    writer = IndexWriter(store_dir,writerConfig)\n",
    "    \n",
    "    #buiild field type\n",
    "    doc_type = FieldType()\n",
    "    doc_type.setStored(True)\n",
    "    doc_type.setTokenized(True)\n",
    "    doc_type.setIndexOptions(IndexOptions.DOCS_AND_FREQS)\n",
    "    id_type = FieldType()\n",
    "    id_type.setStored(True)\n",
    "    id_type.setTokenized(False)\n",
    "    id_type.setIndexOptions(IndexOptions.DOCS)\n",
    "    content_type = FieldType()\n",
    "    content_type.setStored(True)\n",
    "    content_type.setTokenized(True)\n",
    "    content_type.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS)\n",
    "    \n",
    "    #iterate files\n",
    "    for wiki_top, dirnames, filenames in os.walk(wiki_doc_dir):\n",
    "        for filename in filenames:\n",
    "            if not filename.endswith(\".txt\"):\n",
    "                continue\n",
    "            file = open(os.path.join(wiki_top,filename))\n",
    "            sentences = file.readlines()\n",
    "            file.close()\n",
    "            for sentence in sentences:\n",
    "                sentence_list = sentence.strip().split(\" \",2)\n",
    "                try:\n",
    "                    senten_num = int(sentence_list[1]) \n",
    "                except ValueError:\n",
    "                    #senten_num doesn't exist, \n",
    "                    #data format is wrong and should be given up;\n",
    "                    #sum of valid sentence: 25247896\n",
    "                    #sum of total sentences in wiki_doc: 25248397\n",
    "                    continue\n",
    "                wiki_name = sentence_list[0]\n",
    "                wiki_name = ' '.join(wiki_name.split('_'))\n",
    "                senten_cont = sentence_list[2]\n",
    "                wiki_content[wiki_name][str(senten_num)] = senten_cont\n",
    "                doc = Document()\n",
    "                doc.add(Field(\"wiki_name\",wiki_name,doc_type))\n",
    "                doc.add(Field(\"sentence_id\",str(senten_num),id_type))\n",
    "                doc.add(Field(\"content\",senten_cont,content_type))\n",
    "                writer.addDocument(doc)\n",
    "    print(\"%d wiki items in index\" % (writer.numDocs()))\n",
    "    writer.close()\n",
    "    \n",
    "\n",
    "#used for selected wiki_set to construct an index smaller than the whole one\n",
    "def make_target_wiki_index(wiki_set, index_dir, analyzer):\n",
    "    if not os.path.exists(index_dir):\n",
    "        os.mkdir(index_dir)\n",
    "    store_dir = SimpleFSDirectory(File(index_dir).toPath())\n",
    "    writerConfig = IndexWriterConfig(StandardAnalyzer())\n",
    "    #every time start this function, the index will be built from the beginning\n",
    "    writerConfig.setOpenMode(IndexWriterConfig.OpenMode.CREATE)\n",
    "    writer = IndexWriter(store_dir,writerConfig)\n",
    "    \n",
    "    #buiild field type\n",
    "    doc_type = FieldType()\n",
    "    doc_type.setStored(True)\n",
    "    doc_type.setTokenized(True)\n",
    "    doc_type.setIndexOptions(IndexOptions.DOCS_AND_FREQS)\n",
    "    id_type = FieldType()\n",
    "    id_type.setStored(True)\n",
    "    id_type.setTokenized(False)\n",
    "    id_type.setIndexOptions(IndexOptions.DOCS)\n",
    "    content_type = FieldType()\n",
    "    content_type.setStored(True)\n",
    "    content_type.setTokenized(True)\n",
    "    content_type.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS)\n",
    "    \n",
    "    final_names = set()\n",
    "    #iterate wiki_set and remove repeated names\n",
    "    for wiki_names in wiki_set.values():\n",
    "        final_names.update(wiki_names)\n",
    "    for wiki_name in final_names:\n",
    "        for sentence_id,content in wiki_content[wiki_name].items():\n",
    "            doc = Document()\n",
    "            doc.add(Field(\"wiki_name\",wiki_name,doc_type))\n",
    "            doc.add(Field(\"sentence_id\",sentence_id,id_type))\n",
    "            doc.add(Field(\"content\",content,content_type))\n",
    "            writer.addDocument(doc)\n",
    "    print(\"%d wiki items in index\" % (writer.numDocs()))\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create index every time run the following code\n",
    "lucene.initVM()\n",
    "index_dir = \"wiki_index\"\n",
    "wiki_dir = \"wiki-pages-text\"\n",
    "#analyzer which may need to optimize\n",
    "analyzer = StandardAnalyzer()\n",
    "#store whole wiki_content for next index construction\n",
    "wiki_content = defaultdict(dict) # key = wiki_name, value = dict[sentence_id]:content\n",
    "\n",
    "start = datetime.now()\n",
    "make_wiki_index(index_dir, wiki_dir, analyzer, wiki_content)\n",
    "end = datetime.now()\n",
    "print(\"Total wiki index used time: \" + str(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pylucene index query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from org.apache.lucene.index import DirectoryReader,Term\n",
    "from org.apache.lucene.search import IndexSearcher\n",
    "from java.io import File\n",
    "from org.apache.lucene.analysis.standard import StandardAnalyzer\n",
    "from org.apache.lucene.queryparser.classic import QueryParser\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "from allennlp.models.archival import load_archive\n",
    "from allennlp.data.tokenizers import word_filter\n",
    "\n",
    "def doc_search(word_seq, searched_field, searcher, analyzer):\n",
    "    queryParser = QueryParser(searched_field, analyzer)\n",
    "    query = queryParser.parse(word_seq)\n",
    "    \n",
    "    #store returned wiki_name set\n",
    "    wiki_set = set()\n",
    "    \n",
    "    # Run the query and get top 80(?) results\n",
    "    topDocs = searcher.search(query, 80)\n",
    "    #print(\"%s total matching documents.\" % len(topDocs.scoreDocs))\n",
    "    for scoreDoc in topDocs.scoreDocs:\n",
    "        doc = searcher.doc(scoreDoc.doc) #index by id=scoreDoc.doc\n",
    "        wiki_set.add(doc.get(\"wiki_name\"))\n",
    "    return wiki_set\n",
    "\n",
    "#used for query of entire claim sentence(remove stopwords) to get TOPN sentences should return wiki_name and sentence_id\n",
    "def sentence_search(claim, top_x, searcher, analyzer):\n",
    "    #remove stopwords\n",
    "#     sw_filter = word_filter.StopwordFilter()    \n",
    "#     print(claim.split(\" \"))\n",
    "#     filtered_list = sw_filter.filter_words(claim.split(\" \"))\n",
    "#     filtered_sentence = \" \".join(filtered_list)\n",
    "    queryParser = QueryParser('content', analyzer)\n",
    "#     query = queryParser.parse(filtered_sentence)\n",
    "    query = queryParser.parse(claim)\n",
    "    #store returned (wiki_name,sentence_id) tuples\n",
    "    sentence_list = []\n",
    "    \n",
    "    # Run the query and get top x results(for train/dev:30; test:50)\n",
    "    topDocs = searcher.search(query, top_x)\n",
    "    #print(\"%s total matching documents.\" % len(topDocs.scoreDocs))\n",
    "    for scoreDoc in topDocs.scoreDocs:\n",
    "        doc = searcher.doc(scoreDoc.doc) #index by id=scoreDoc.doc\n",
    "        sentence_list.append([doc.get(\"wiki_name\"),doc.get(\"sentence_id\")])\n",
    "    return sentence_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER model construction: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NER_modeling(predictor, sentence): #construct open Information Extraction model\n",
    "    NER_predicted = predictor.predict(sentence)\n",
    "    NER_results = []\n",
    "    entity = \"\"\n",
    "    for idx, tag in enumerate(NER_predicted['tags']):\n",
    "        if tag != 'O': #need to store\n",
    "            entity = \" \".join((entity,NER_predicted['words'][idx]))\n",
    "        else:\n",
    "            if entity != \"\":\n",
    "                entity = re.sub(\"\\[\",\"-LSB- \",entity)\n",
    "                entity = re.sub(\"]\",\" -RSB-\",entity)\n",
    "                entity = re.sub(\"\\(\",\"-LRB- \",entity)\n",
    "                entity = re.sub(\"\\)\",\" -RRB-\",entity)\n",
    "                entity = re.sub(\":\",\"-COLON-\",entity)\n",
    "                entity = re.sub(\"\\\"\",\"\",entity)\n",
    "                entity = re.sub(\"\\/\",\"\",entity)\n",
    "                entity = re.sub(\"!\",\"\",entity)\n",
    "                NER_results.append(entity.strip())\n",
    "                entity = \"\"\n",
    "    if entity != \"\":\n",
    "        entity = re.sub(\"\\[\",\"-LSB- \",entity)\n",
    "        entity = re.sub(\"]\",\" -RSB-\",entity)\n",
    "        entity = re.sub(\"\\(\",\"-LRB- \",entity)\n",
    "        entity = re.sub(\"\\)\",\" -RRB-\",entity)\n",
    "        entity = re.sub(\":\",\"-COLON-\",entity)\n",
    "        entity = re.sub(\"\\\"\",\"\",entity)\n",
    "        entity = re.sub(\"\\/\",\"\",entity)\n",
    "        entity = re.sub(\"!\",\"\",entity)\n",
    "        NER_results.append(entity.strip())\n",
    "    return NER_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third, Searching for train/dev/test set to narrow down candidate wikis and sentences:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.First round of searching 'content'&'wiki_name' Field for wiki_document (using NER). Keep recall of wikis high to ensure most docs are included in, so won't miss too many wikis for test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainset as what dev do\n",
    "store_FSD = SimpleFSDirectory(File(index_dir).toPath())\n",
    "reader = DirectoryReader.open(store_FSD)\n",
    "searcher = IndexSearcher(reader)\n",
    "analyzer = StandardAnalyzer()\n",
    "#GPU mode\n",
    "#archive = load_archive(\"https://s3-us-west-2.amazonaws.com/allennlp/models/fine-grained-ner-model-elmo-2018.12.21.tar.gz\", cuda_device=0)\n",
    "NER_archive = load_archive(\"https://s3-us-west-2.amazonaws.com/allennlp/models/ner-model-2018.12.18.tar.gz\", cuda_device=0)\n",
    "NER_predictor = Predictor.from_archive(NER_archive)\n",
    "\n",
    "#store filtered wiki_name for data, key is corresponding claim  id\n",
    "train_wikis = dict()\n",
    "\n",
    "t1 = datetime.now()\n",
    "\n",
    "#train_set search NER phrases\n",
    "for idx,data in train_set.items():\n",
    "    #store filtered wiki_name(potential evidence within which)\n",
    "    evident_wikis = set()\n",
    "    NER_claim = NER_modeling(NER_predictor,data['claim'])\n",
    "    for ner in NER_claim:\n",
    "        evident_wikis.update(doc_search(ner, \"content\", searcher, analyzer))\n",
    "        evident_wikis.update(doc_search(ner, \"wiki_name\", searcher, analyzer))\n",
    "        #改成每个取TOP100,然后对所有claim的返回值取交集？\n",
    "    train_wikis[idx] = evident_wikis\n",
    "del searcher\n",
    "t2 = datetime.now()\n",
    "print(\"Total used time: \",t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dev set\n",
    "store_FSD = SimpleFSDirectory(File(index_dir).toPath())\n",
    "reader = DirectoryReader.open(store_FSD)\n",
    "searcher = IndexSearcher(reader)\n",
    "analyzer = StandardAnalyzer()\n",
    "#GPU mode\n",
    "#archive = load_archive(\"https://s3-us-west-2.amazonaws.com/allennlp/models/fine-grained-ner-model-elmo-2018.12.21.tar.gz\", cuda_device=0)\n",
    "NER_archive = load_archive(\"https://s3-us-west-2.amazonaws.com/allennlp/models/ner-model-2018.12.18.tar.gz\", cuda_device=0)\n",
    "NER_predictor = Predictor.from_archive(NER_archive)\n",
    "\n",
    "#store filtered wiki_name for data, key is corresponding claim  id\n",
    "dev_wikis = dict()\n",
    "\n",
    "t1 = datetime.now()\n",
    "\n",
    "#dev_set search NER phrases\n",
    "for idx,data in dev_set.items():\n",
    "    #store filtered wiki_name(potential evidence within which)\n",
    "    evident_wikis = set()\n",
    "\n",
    "    NER_claim = NER_modeling(NER_predictor,data['claim'])\n",
    "    for ner in NER_claim:\n",
    "        evident_wikis.update(doc_search(ner, \"content\", searcher, analyzer))\n",
    "        evident_wikis.update(doc_search(ner, \"wiki_name\", searcher, analyzer))\n",
    "    dev_wikis[idx] = evident_wikis        \n",
    "        \n",
    "del searcher\n",
    "t2 = datetime.now()\n",
    "print(\"Total used time: \",t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testset as what dev do\n",
    "store_FSD = SimpleFSDirectory(File(index_dir).toPath())\n",
    "reader = DirectoryReader.open(store_FSD)\n",
    "searcher = IndexSearcher(reader)\n",
    "analyzer = StandardAnalyzer()\n",
    "#GPU mode\n",
    "NER_archive = load_archive(\"https://s3-us-west-2.amazonaws.com/allennlp/models/ner-model-2018.12.18.tar.gz\", cuda_device=0)\n",
    "NER_predictor = Predictor.from_archive(NER_archive)\n",
    "\n",
    "#store filtered wiki_name for data, key is corresponding claim  id\n",
    "test_wikis = dict()\n",
    "\n",
    "t1 = datetime.now()\n",
    "\n",
    "#test_set search NER phrases\n",
    "for idx,data in test_set.items():\n",
    "    #store filtered wiki_name(potential evidence within which)\n",
    "    evident_wikis = set()\n",
    "    NER_claim = NER_modeling(NER_predictor,data['claim'])\n",
    "    for ner in NER_claim:\n",
    "        evident_wikis.update(doc_search(ner, \"content\", searcher, analyzer))\n",
    "        evident_wikis.update(doc_search(ner, \"wiki_name\", searcher, analyzer))\n",
    "    test_wikis[idx] = evident_wikis\n",
    "        \n",
    "del searcher\n",
    "t2 = datetime.now()\n",
    "print(\"Total used time: \",t2-t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate recall of correct document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recall of correct document\n",
    "def doc_recall(data_set,wiki_list):\n",
    "    correct_num = 0.0\n",
    "    for idx,data in data_set.items():\n",
    "        correct_evi = 0\n",
    "        for evidence in data['evidence']:\n",
    "            if \" \".join(evidence[0].split(\"_\")) in wiki_list[idx]:\n",
    "                correct_evi += 1\n",
    "        if correct_evi == len(data['evidence']):\n",
    "            correct_num += 1\n",
    "    return correct_num/len(data_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(doc_recall(train_set,train_wikis))\n",
    "print(doc_recall(dev_set,dev_wikis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Second round of searching 'content' Field for TopX sentences (using 'claim' with stopwords removed). Still, keep the recall of sentences high, so test may get most of correct sentences when using this data as input of BERT in the following period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.Construct index of selected wiki_name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence index for train!!!\n",
    "train_index_dir = \"sentence_index_train\"\n",
    "#analyzer which may need to optimize\n",
    "analyzer = StandardAnalyzer()\n",
    "\n",
    "start = datetime.now()\n",
    "make_target_wiki_index(train_wikis, train_index_dir, analyzer)\n",
    "end = datetime.now()\n",
    "print(\"Selected-wiki-index used time: \" + str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sentence index for dev!!!\n",
    "dev_index_dir = \"sentence_index_dev\"\n",
    "#analyzer which may need to optimize\n",
    "analyzer = StandardAnalyzer()\n",
    "\n",
    "start = datetime.now()\n",
    "make_target_wiki_index(dev_wikis, dev_index_dir, analyzer)\n",
    "end = datetime.now()\n",
    "print(\"Selected-wiki-index used time: \" + str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence index for test!!!\n",
    "test_index_dir = \"sentence_index_test\"\n",
    "#analyzer which may need to optimize\n",
    "analyzer = StandardAnalyzer()\n",
    "\n",
    "start = datetime.now()\n",
    "make_target_wiki_index(test_wikis, test_index_dir, analyzer)\n",
    "end = datetime.now()\n",
    "print(\"Selected-wiki-index used time: \" + str(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.Searching sentences: (using claim removed stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english') + [\"{\",\"}\",\"[\",\"]\",\"(\",\")\",\"/\",\",\", '.', ':', '!', ';', \"'\", '\"', '&', '$', '#', '@', '?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainset\n",
    "store_FSD = SimpleFSDirectory(File(train_index_dir).toPath())\n",
    "reader = DirectoryReader.open(store_FSD)\n",
    "searcher = IndexSearcher(reader)\n",
    "analyzer = StandardAnalyzer()\n",
    "train_sentences = dict()\n",
    "t1 = datetime.now()\n",
    "\n",
    "#train_set search claim\n",
    "for idx,data in train_set.items():\n",
    "    claim_tokens = word_tokenize(data['claim'])\n",
    "    filtered_sentence = \" \".join([w for w in claim_tokens if not w in stop_words])\n",
    "    filtered_sentence = re.sub(\"\\\"\",\"\",filtered_sentence)\n",
    "    filtered_sentence = re.sub(\"/\",\"\",filtered_sentence)\n",
    "    filtered_sentence = re.sub(\":\",\"\",filtered_sentence)\n",
    "    train_sentences[idx] = sentence_search(filtered_sentence, 30, searcher, analyzer)\n",
    "\n",
    "del searcher\n",
    "t2 = datetime.now()\n",
    "print(\"Total used time: \",t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#devset\n",
    "store_FSD = SimpleFSDirectory(File(dev_index_dir).toPath())\n",
    "reader = DirectoryReader.open(store_FSD)\n",
    "searcher = IndexSearcher(reader)\n",
    "analyzer = StandardAnalyzer()\n",
    "dev_sentences = dict()\n",
    "t1 = datetime.now()\n",
    "\n",
    "#dev_set search claim\n",
    "for idx,data in dev_set.items():\n",
    "    claim_tokens = word_tokenize(data['claim'])\n",
    "    filtered_sentence = \" \".join([w for w in claim_tokens if not w in stop_words])\n",
    "    filtered_sentence = re.sub(\"\\\"\",\"\",filtered_sentence)\n",
    "    filtered_sentence = re.sub(\"/\",\"\",filtered_sentence)\n",
    "    filtered_sentence = re.sub(\":\",\"\",filtered_sentence)\n",
    "    dev_sentences[idx] = sentence_search(filtered_sentence, 30, searcher, analyzer)\n",
    "\n",
    "del searcher\n",
    "t2 = datetime.now()\n",
    "print(\"Total used time: \",t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_set\n",
    "store_FSD = SimpleFSDirectory(File(test_index_dir).toPath())\n",
    "reader = DirectoryReader.open(store_FSD)\n",
    "searcher = IndexSearcher(reader)\n",
    "analyzer = StandardAnalyzer()\n",
    "test_sentences = dict()\n",
    "t1 = datetime.now()\n",
    "\n",
    "#test_set search claim\n",
    "for idx,data in test_set.items():\n",
    "    claim_tokens = word_tokenize(data['claim'])\n",
    "    filtered_sentence = \" \".join([w for w in claim_tokens if not w in stop_words])\n",
    "    filtered_sentence = re.sub(\"\\\"\",\"\",filtered_sentence)\n",
    "    filtered_sentence = re.sub(\"/\",\"\",filtered_sentence)\n",
    "    filtered_sentence = re.sub(\":\",\"\",filtered_sentence)\n",
    "    test_sentences[idx] = sentence_search(filtered_sentence, 30, searcher, analyzer)\n",
    "\n",
    "del searcher\n",
    "t2 = datetime.now()\n",
    "print(\"Total used time: \",t2-t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate recall of correct sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recall of correct document\n",
    "def second_round_doc_recall(data_set,sentence_list):\n",
    "    correct_num = 0.0\n",
    "    for idx,data in data_set.items():\n",
    "        correct_evi = 0\n",
    "        for evidence in data['evidence']:\n",
    "            if [\" \".join(evidence[0].split(\"_\")),str(evidence[1])] in sentence_list[idx]:\n",
    "                correct_evi += 1\n",
    "        if correct_evi == len(data['evidence']):\n",
    "            correct_num += 1\n",
    "    return correct_num/len(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(second_round_doc_recall(train_set,train_sentences))\n",
    "print(second_round_doc_recall(dev_set,dev_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourth, Train model using BERT:\n",
    "(see the training and predicting part in \"wsta-train-model.ipynb\" which is run in Colaboratory):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format input of BERT train set for sentence selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT model 的train输入项\n",
    "train = dict()\n",
    "for idx in train_set.keys():\n",
    "    train[idx] = dict()\n",
    "    train[idx]['claim'] = train_set[idx]['claim']\n",
    "    train[idx]['evidence'] = defaultdict(dict)\n",
    "    for sentence in train_set[idx]['evidence']:\n",
    "        train[idx]['evidence'][\" \".join(sentence[0].split(\"_\"))][str(sentence[1])] \\\n",
    "            = [wiki_content[\" \".join(sentence[0].split(\"_\"))][str(sentence[1])],\"Positive\"]\n",
    "    for sentence in train_sentences[idx]:\n",
    "        if sentence[0] in train[idx]['evidence'].keys():\n",
    "            if sentence[1] in train[idx]['evidence'][sentence[0]].keys():\n",
    "                continue\n",
    "        train[idx]['evidence'][sentence[0]][sentence[1]] = [wiki_content[sentence[0]][sentence[1]],\"Negative\"]\n",
    "\n",
    "with open(\"trainset_for_train.json\",\"w\",encoding = 'utf-8') as f: # store half-processed file for safe\n",
    "    json.dump(train,f,indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"trainset_for_train.json\",\"r\",encoding = 'utf-8') as f:\n",
    "    train = json.load(f)\n",
    "    \n",
    "train_list = [] #output data\n",
    "train_index = [] #store corresponding idx,wiki_name,sentence_id of dev_list item\n",
    "for idx in train.keys():\n",
    "    count = 0\n",
    "    for wiki_name in train[idx]['evidence'].keys():\n",
    "        for sentence_id in train[idx]['evidence'][wiki_name].keys():\n",
    "            label = 1\n",
    "            if train[idx]['evidence'][wiki_name][sentence_id][1] == \"Negative\":\n",
    "                count+=1\n",
    "                label = 0\n",
    "            train_list.append([train[idx]['claim'],wiki_name + \" , \" + train[idx]['evidence'][wiki_name][sentence_id][0],label])\n",
    "            train_index.append([idx,\"_\".join(wiki_name.split(\" \")),sentence_id])\n",
    "            if count == 5:\n",
    "                break\n",
    "        if count == 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trainset_for_model.tsv', 'wt') as fp:\n",
    "    tsv_writer = csv.writer(fp, delimiter='\\t')\n",
    "    for row in train_list:\n",
    "        tsv_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format input of BERT dev set for sentence selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT model 的dev输入项\n",
    "dev = dict()\n",
    "for idx in dev_set.keys():\n",
    "    dev[idx] = dict()\n",
    "    dev[idx]['claim'] = dev_set[idx]['claim']\n",
    "    dev[idx]['evidence'] = defaultdict(dict)\n",
    "    for sentence in dev_set[idx]['evidence']:\n",
    "        dev[idx]['evidence'][\" \".join(sentence[0].split(\"_\"))][str(sentence[1])] \\\n",
    "            = [wiki_content[\" \".join(sentence[0].split(\"_\"))][str(sentence[1])],\"Positive\"]\n",
    "    for sentence in dev_sentences[idx]:\n",
    "        if sentence[0] in dev[idx]['evidence'].keys():\n",
    "            if sentence[1] in dev[idx]['evidence'][sentence[0]].keys():\n",
    "                continue\n",
    "        dev[idx]['evidence'][sentence[0]][sentence[1]] = [wiki_content[sentence[0]][sentence[1]],\"Negative\"]\n",
    "with open(\"devset_for_train.json\",\"w\",encoding = 'utf-8') as f: # store half-processed file for safe\n",
    "    json.dump(dev,f,indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"devset_for_train.json\",\"r\",encoding = 'utf-8') as f:\n",
    "    dev = json.load(f)\n",
    "\n",
    "dev_list = [] #output data\n",
    "dev_index = [] #store corresponding idx,wiki_name,sentence_id of dev_list item\n",
    "for idx in dev.keys():\n",
    "    count = 0\n",
    "    for wiki_name in dev[idx]['evidence'].keys():\n",
    "        for sentence_id in dev[idx]['evidence'][wiki_name].keys():\n",
    "            label = 1\n",
    "            if dev[idx]['evidence'][wiki_name][sentence_id][1] == \"Negative\":\n",
    "                count+=1\n",
    "                label = 0\n",
    "            dev_list.append([dev[idx]['claim'],wiki_name + \" , \" + dev[idx]['evidence'][wiki_name][sentence_id][0],label])\n",
    "            dev_index.append([idx,\"_\".join(wiki_name.split(\" \")),sentence_id])\n",
    "            if count == 5:\n",
    "                break\n",
    "        if count == 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('devset_for_model.tsv', 'wt') as fp:\n",
    "    tsv_writer = csv.writer(fp, delimiter='\\t')\n",
    "    for row in dev_list:\n",
    "        tsv_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format input of BERT test set for sentence selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT model 的test输入项\n",
    "test = dict()\n",
    "for idx in test_set.keys():\n",
    "    test[idx] = dict()\n",
    "    test[idx]['claim'] = test_set[idx]['claim']\n",
    "    test[idx]['evidence'] = defaultdict(dict)\n",
    "    for sentence in test_sentences[idx]:\n",
    "        test[idx]['evidence'][sentence[0]][sentence[1]] = wiki_content[sentence[0]][sentence[1]]\n",
    "with open(\"testset_for_train.json\",\"w\",encoding = 'utf-8') as f:\n",
    "    json.dump(test,f,indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"testset_for_train.json\",\"r\",encoding = 'utf-8') as f:\n",
    "    test = json.load(f)\n",
    "test_list = [] #output data\n",
    "test_index = [] #store corresponding idx,wiki_name,sentence_id of dev_list item\n",
    "for idx in test.keys():\n",
    "    for wiki_name in test[idx]['evidence'].keys():\n",
    "        for sentence_id in test[idx]['evidence'][wiki_name].keys():\n",
    "            test_list.append([test[idx]['claim'],wiki_name + \" , \" + test[idx]['evidence'][wiki_name][sentence_id]])\n",
    "            test_index.append([idx,\"_\".join(wiki_name.split(\" \")),sentence_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('testset_for_model.tsv', 'wt') as fp:\n",
    "    tsv_writer = csv.writer(fp, delimiter='\\t')\n",
    "    for row in test_list:\n",
    "        tsv_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"content_to_index_test.json\",\"w\",encoding = 'utf-8') as f:\n",
    "    for row in test_index:\n",
    "        f.write(row[0]+\" \"+row[1]+\" \"+row[2]+\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Due to delayed submission of codalab, this part of code alternatively uses dev set as test set for alternative evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"devset_for_train.json\",\"r\",encoding = 'utf-8') as f:\n",
    "    dev = json.load(f)\n",
    "\n",
    "dev_list_for_test = [] #output data\n",
    "dev_index_for_test = [] #store corresponding idx,wiki_name,sentence_id of dev_list item\n",
    "\n",
    "for idx in dev.keys():\n",
    "    for wiki_name in dev[idx]['evidence'].keys():\n",
    "        for sentence_id in dev[idx]['evidence'][wiki_name].keys():\n",
    "            dev_list_for_test.append([dev[idx]['claim'],wiki_name + \" , \" + dev[idx]['evidence'][wiki_name][sentence_id][0]])\n",
    "            dev_index_for_test.append([idx,\"_\".join(wiki_name.split(\" \")),sentence_id])\n",
    "with open('devset_usedfortest_for_model.tsv', 'wt') as fp:\n",
    "    tsv_writer = csv.writer(fp, delimiter='\\t')\n",
    "    for row in dev_list_for_test:\n",
    "        tsv_writer.writerow(row)\n",
    "with open(\"content_to_index_devastest.json\",\"w\",encoding = 'utf-8') as f:\n",
    "    for row in dev_index_for_test:\n",
    "        f.write(row[0]+\" \"+row[1]+\" \"+row[2]+\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification part preprocess:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deal with the output of sentence selection for test set from colab notebook ："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_sentence_probs = [] \n",
    "with open(\"selection_test_results.txt\",\"r\",encoding='utf-8') as f:\n",
    "    for row in f.readlines():\n",
    "        temp = row.strip(\" \\n][\").split(\" \")\n",
    "        ts_probs = [float(temp[0]),float(temp[-1])]\n",
    "        test_sentence_probs.append(ts_probs)\n",
    "for idx,data in test_set.items():\n",
    "    test_set[idx]['evidence'] = []\n",
    "for idx,probs in enumerate(test_sentence_probs):\n",
    "    if probs[0]>=0.998:\n",
    "        test_set[test_index[idx][0]]['evidence'].append([test_index[idx][1],int(test_index[idx][2])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Due to delayed submission of codalab, this part of code alternatively uses dev set as test set for alternative evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_sentence_probs = []\n",
    "#new a dev set for testing\n",
    "new_dev_set = dict()\n",
    "with open(\"selection_devastest_results.txt\",\"r\",encoding='utf-8') as f:\n",
    "    for row in f.readlines():\n",
    "        temp = row.strip(\" \\n][\").split(\" \")\n",
    "        ts_probs = [float(temp[0]),float(temp[-1])]\n",
    "        dev_sentence_probs.append(ts_probs)\n",
    "for idx,data in dev_set.items():\n",
    "    new_dev_set[idx] = dict()\n",
    "    new_dev_set[idx]['claim'] = dev_set[idx]['claim']\n",
    "    new_dev_set[idx]['label'] = \"\"\n",
    "    new_dev_set[idx]['evidence'] = []\n",
    "for idx,probs in enumerate(dev_sentence_probs):\n",
    "    if probs[0]>=0.99:\n",
    "        new_dev_set[dev_index_for_test[idx][0]]['evidence'].append([dev_index_for_test[idx][1],int(dev_index_for_test[idx][2])])\n",
    "with open(\"new_dev_set_for_test.json\",\"w\",encoding = 'utf-8') as f: # store dev file without label\n",
    "    json.dump(new_dev_set,f,indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge all evidence sentences in one evidence："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_list = []#for classification\n",
    "for idx in train_set.keys():\n",
    "    final_sentence = \"\"\n",
    "    for sentence_info in train_set[idx]['evidence']:\n",
    "        final_sentence = final_sentence + \" \" + wiki_content[' '.join(sentence_info[0].split('_'))][str(sentence_info[1])]\n",
    "    final_train_list.append([train_set[idx]['claim'],final_sentence,train_set[idx]['label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_classification.tsv', 'wt') as fp:\n",
    "    tsv_writer = csv.writer(fp, delimiter='\\t')\n",
    "    for row in final_train_list:\n",
    "        tsv_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dev set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dev_list = []#for classification\n",
    "for idx in dev_set.keys(): \n",
    "    final_sentence = \"\"\n",
    "    for sentence_info in dev_set[idx]['evidence']:\n",
    "        final_sentence = final_sentence + \" \" + wiki_content[' '.join(sentence_info[0].split('_'))][str(sentence_info[1])]\n",
    "    final_dev_list.append([dev_set[idx]['claim'],final_sentence,dev_set[idx]['label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dev_classification.tsv', 'wt') as fp:\n",
    "    tsv_writer = csv.writer(fp, delimiter='\\t')\n",
    "    for row in final_dev_list:\n",
    "        tsv_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_list = []#for classification\n",
    "final_test_index = []\n",
    "for idx in test_set.keys(): \n",
    "    final_sentence = \"\"\n",
    "    for sentence_info in test_set[idx]['evidence']:\n",
    "        final_sentence = final_sentence + \" \" + wiki_content[' '.join(sentence_info[0].split('_'))][str(sentence_info[1])]\n",
    "    final_test_list.append([test_set[idx]['claim'],final_sentence])\n",
    "    final_test_index.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_classification.tsv', 'wt') as fp:\n",
    "    tsv_writer = csv.writer(fp, delimiter='\\t')\n",
    "    for row in final_test_list:\n",
    "        tsv_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Due to delayed submission of codalab, this part of code alternatively uses dev set as test set for alternative evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_devastest_list = []#for classification\n",
    "final_devastest_index = []\n",
    "for idx in new_dev_set.keys(): \n",
    "    final_sentence = \"\"\n",
    "    for sentence_info in new_dev_set[idx]['evidence']:\n",
    "        final_sentence = final_sentence + \" \" + wiki_content[' '.join(sentence_info[0].split('_'))][str(sentence_info[1])]\n",
    "    final_devastest_list.append([new_dev_set[idx]['claim'],final_sentence])\n",
    "    final_devastest_index.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('devastest_classification.tsv', 'wt') as fp:\n",
    "    tsv_writer = csv.writer(fp, delimiter='\\t')\n",
    "    for row in final_devastest_list:\n",
    "        tsv_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration and output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label_probs = []\n",
    "with open(\"classification_test_results.txt\",\"r\",encoding='utf-8') as f:\n",
    "    for row in f.readlines():\n",
    "        temp = row.strip(\" \\n][\").split()\n",
    "        ts_probs = [float(temp[0]),float(temp[1]),float(temp[2])]\n",
    "        test_label_probs.append(ts_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_label = [\"SUPPORTS\",\"REFUTES\",\"NOT ENOUGH INFO\"]\n",
    "for idx,probs in enumerate(test_label_probs):\n",
    "    label_idx = probs.index(max(probs))\n",
    "    test_set[final_test_index[idx]]['label'] = temp_label[label_idx]\n",
    "with open(\"testoutput.json\",\"w\",encoding = 'utf-8') as f:\n",
    "    json.dump(test_set,f,indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Due to delayed submission of codalab, this part of code alternatively uses dev set as test set for alternative evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_label_probs = []\n",
    "with open(\"classification_devastest_results.txt\",\"r\",encoding='utf-8') as f:\n",
    "    for row in f.readlines():\n",
    "        temp = row.strip(\" \\n][\").split()\n",
    "        ts_probs = [float(temp[0]),float(temp[1]),float(temp[2])]\n",
    "        dev_label_probs.append(ts_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_label = [\"SUPPORTS\",\"REFUTES\",\"NOT ENOUGH INFO\"]\n",
    "for idx,probs in enumerate(dev_label_probs):\n",
    "    label_idx = probs.index(max(probs))\n",
    "    if len(new_dev_set[final_devastest_index[idx]]['evidence']) == 0:\n",
    "        new_dev_set[final_devastest_index[idx]]['label'] = \"NOT ENOUGH INFO\"\n",
    "with open(\"devastest_final_result.json\",\"w\",encoding = 'utf-8') as f:\n",
    "    json.dump(new_dev_set,f,indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --------------------------------------------------------- END ------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
